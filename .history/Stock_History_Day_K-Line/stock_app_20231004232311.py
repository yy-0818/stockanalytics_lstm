import json
import logging

import os
import streamlit as st
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
import tensorflow as tf
# import tensorflow_probability as tfp
from keras.models import load_model
from streamlit_pandas_profiling import st_profile_report
from PIL import Image

sns.set(style='whitegrid', font='SimHei')
st.set_page_config(
    layout="wide",
    page_title='Real-Time Stock Price Prediction',
    page_icon = 'ğŸ“‹',
)

@st.cache_data 
def load_models():
    model_paths = [os.getcwd() + '\Stock_History_Day_K-Line\Model\model_{}.h5'.format(n) for n in range(1, 3)]
    models = [load_model(model_path) for model_path in model_paths]
    return models

@st.cache_data 
def load_data():
    df_locs = [os.getcwd() + '\Stock_History_Day_K-Line\Data\stock_{}.csv'.format(n) for n in range(1, 4)]
    dfs = [pd.read_csv(df_loc) for df_loc in df_locs]
    return dfs

# åŠ è½½æ•°æ®
stock_data = load_data()
moutai_stock = stock_data[0]
aapl_stock = stock_data[1]
tencent_stock = stock_data[2]
# åŠ è½½æ¨¡å‹æ•°æ®
models = load_models()
rnn_model = models[0]
lstm_model = models[1]

def main(df):
    # st.title('åŸºäº LSTM æ¨¡å‹è‚¡ç¥¨åˆ†æä¸é¢„æµ‹ğŸ“ˆ')
    st.markdown("<h1 style='text-align: center; color: black;'>åŸºäº LSTM æ¨¡å‹è‚¡ç¥¨åˆ†æä¸é¢„æµ‹ğŸ“ˆ</h1>", unsafe_allow_html=True)
    st.markdown("<h4 style='text-align: center; color: black;'>Stock Market Analysis + Prediction using LSTMğŸ“ˆ</h4>", unsafe_allow_html=True)

    st.subheader('by YuanKai ([@ivan](https://blog.ivanlife.cn/))')

    st.markdown(
    """
    
    <br><br/>
    è¿‘å¹´æ¥ï¼Œè‚¡ç¥¨é¢„æµ‹è¿˜å¤„äºä¸€ä¸ªå¾ˆçƒ­é—¨çš„é˜¶æ®µï¼Œå› ä¸ºè‚¡ç¥¨å¸‚åœºçš„æ³¢åŠ¨ååˆ†å·¨å¤§ï¼Œéšæ—¶å¯èƒ½å› ä¸ºä¸€äº›æ–°çš„æ”¿ç­–æˆ–è€…å…¶ä»–åŸå› ï¼Œè¿›è¡Œå¤§å¹…åº¦çš„æ³¢åŠ¨ï¼Œå¯¼è‡´è‡ªç„¶äººè‚¡æ°‘å¾ˆéš¾å¯¹è‚¡ç¥¨è¿›è¡ŒæŠ•èµ„ç›ˆåˆ©ã€‚

    å› æ­¤æœ¬æ–‡æƒ³åˆ©ç”¨ç°æœ‰çš„æ¨¡å‹ä¸ç®—æ³•ï¼Œå¯¹è‚¡ç¥¨ä»·æ ¼è¿›è¡Œé¢„æµ‹ï¼Œä»è€Œä½¿è‡ªç„¶äººè‚¡æ°‘å¯ä»¥è‡ªå·±å¯¹è‚¡ç¥¨è¿›è¡Œé¢„æµ‹ã€‚

    ç†è®ºä¸Šï¼Œè‚¡ç¥¨ä»·æ ¼æ˜¯å¯ä»¥é¢„æµ‹çš„ï¼Œä½†æ˜¯å½±å“è‚¡ç¥¨ä»·æ ¼çš„å› ç´ æœ‰å¾ˆå¤šï¼Œè€Œä¸”ç›®å‰ä¸ºæ­¢ï¼Œå®ƒä»¬å¯¹è‚¡ç¥¨çš„å½±å“è¿˜ä¸èƒ½æ¸…æ™°å®šä¹‰ã€‚è¿™æ˜¯å› ä¸ºè‚¡ç¥¨é¢„æµ‹æ˜¯é«˜åº¦éçº¿æ€§çš„ï¼Œè¿™å°±è¦é¢„æµ‹æ¨¡å‹è¦èƒ½å¤Ÿå¤„ç†éçº¿æ€§é—®é¢˜ï¼Œå¹¶ä¸”ï¼Œè‚¡ç¥¨å…·æœ‰æ—¶é—´åºåˆ—çš„ç‰¹æ€§ï¼Œå› æ­¤é€‚åˆç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå¯¹è‚¡ç¥¨è¿›è¡Œé¢„æµ‹ã€‚
    
    è™½ç„¶å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œå…è®¸ä¿¡æ¯çš„æŒä¹…åŒ–ï¼Œç„¶è€Œï¼Œä¸€èˆ¬çš„RNNæ¨¡å‹å¯¹å…·å¤‡é•¿è®°å¿†æ€§çš„æ—¶é—´åºåˆ—æ•°æ®åˆ»ç”»èƒ½åŠ›è¾ƒå¼±ï¼Œåœ¨æ—¶é—´åºåˆ—è¿‡é•¿çš„æ—¶å€™ï¼Œå› ä¸ºå­˜åœ¨æ¢¯åº¦æ¶ˆæ•£å’Œæ¢¯åº¦çˆ†ç‚¸ç°è±¡RNNè®­ç»ƒå˜å¾—éå¸¸å›°éš¾ã€‚Hochreiter å’Œ Schmidhuber æå‡ºçš„é•¿çŸ­æœŸè®°å¿†ï¼ˆ Long Short-Term Memoryï¼ŒLSTMï¼‰æ¨¡å‹åœ¨RNNç»“æ„çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹é€ ï¼Œä»è€Œè§£å†³äº†RNNæ¨¡å‹æ— æ³•åˆ»ç”»æ—¶é—´åºåˆ—é•¿è®°å¿†æ€§çš„é—®é¢˜ã€‚

    """
    , unsafe_allow_html=True)
    should_tell_me_more = st.button('Tell me more')
    if should_tell_me_more:
        tell_me_more()
        st.markdown('---')
    else:
        st.markdown('---')
        interactive_galaxies(df)



def tell_me_more():
    st.title('Building the Model')

    st.button('Back to galaxies')  # will change state and hence trigger rerun and hence reset should_tell_me_more

    st.markdown("""
    We require a model which can:
    - Learn efficiently from volunteer responses of varying (i.e. heteroskedastic) uncertainty
    - Predict posteriors for those responses on new galaxies, for every question

    In [previous work](https://arxiv.org/abs/1905.07424), we modelled volunteer responses as being binomially distributed and trained our model to make maximum likelihood estimates using the loss function:
    """)

    st.latex(
    """
    \mathcal{L} = k \log f^w(x) + (N-k) \log(1-f^w(x))
    """
    )
    st.markdown(
    r"""
    where, for some target question, k is the number of responses (successes) of some target answer, N is the total number of responses (trials) to all answers, and $f^w(x) = \hat{\rho}$ is the predicted probability of a volunteer giving that answer.
    """
    )
   
    st.markdown(
    r"""
    This binomial assumption, while broadly successful, broke down for galaxies with vote fractions k/N close to 0 or 1, where the Binomial likelihood is extremely sensitive to $f^w(x)$, and for galaxies where the question asked was not appropriate (e.g. predict if a featureless galaxy has a bar). 
    
    Instead, in our latest work, the model predicts a distribution 
    """)

    st.latex(r"""
    f^w(x) = p(\rho|f^w(x))
    """)
    
    st.markdown(r"""
    and $\rho$ is then drawn from that distribution.
    
    For binary questions, one could use the Beta distribution (being flexible and defined on the unit interval), and predict the Beta distribution parameters $f^w(x) = (\hat{\alpha}, \hat{\beta})$ by minimising

    """)

    st.latex(
    r"""
        \mathcal{L} = \int Bin(k|\rho, N) Beta(\rho|\alpha, \beta) d\alpha d\beta    
    """
    )
    st.markdown(r"""

    where the Binomial and Beta distributions are conjugate and hence this integral can be evaluated analytically.

    In practice, we would like to predict the responses to questions with more than two answers, and hence we replace each distribution with its multivariate counterpart; Beta($\rho|\alpha, \beta$) with Dirichlet($\vec{\rho}|\vec{\alpha})$, and Binomial($k|\rho, N$) with Multinomial($\vec{k}|\vec{\rho}, N$).
    """)
    
    st.latex(r"""
     \mathcal{L}_q = \int Multi(\vec{k}|\vec{\rho}, N) Dirichlet(\vec{\rho}| \vec{\alpha}) d\vec{\alpha}
    """)
    
    st.markdown(r"""
    where $\vec{k}, \vec{\rho}$ and $\vec{\alpha}$ are now all vectors with one element per answer. 

    Using this loss function, our model can predict posteriors with excellent calibration.

    For the final GZ DECaLS predictions, I actually use an ensemble of models, and apply active learning - picking the galaxies where the models confidently disagree - to choose the most informative galaxies to label with Galaxy Zoo. Check out the paper for more.
    
    """)

    st.button('Back to galaxies', key='back_again')  # will change state and hence trigger rerun and hence reset should_tell_me_more



def interactive_galaxies(df):
    # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå°†æ•°æ®é›†åç§°æ˜ å°„åˆ°æ•°æ®æ¡†
    stock_data = {
        'è´µå·èŒ…å°': moutai_stock,
        'è‹¹æœ': aapl_stock,
        'è…¾è®¯æ§è‚¡': tencent_stock
    }
    stock_model = {
        'RNN': rnn_model,
        'LSTM': lstm_model,
    }
    with st.sidebar:
        st.title('')
        st.image('https://static.ivanlife.cn/img/wallhaven-9mo7kw_1920x1080.png')
        st.markdown('# è®¾ç½®å‚æ•°ğŸ“')
        st.write('User input parameters below â¬‡ï¸')
        
        # åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºé€‰æ‹©æ¡†
        stock_df = st.sidebar.selectbox('é€‰æ‹©æ•°æ®é›†', list(stock_data.keys()))
        # st.sidebar.write('å½“å‰æ•°æ®é›†ä¸ºï¼š', stock_df)
        stock_model = st.sidebar.selectbox('é€‰æ‹©æ¨¡å‹', list(stock_model.keys()))
        st.info('è¯¥é¡¹ç›®å¯ä»¥å¸®åŠ©ä½ ç†è§£LSTM')

    selected_stock_df = stock_data[stock_df]
    # å¦‚æœç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šçš„æ•°æ®é›†ï¼Œåˆ™æ˜¾ç¤ºå¯¹åº”çš„åŸå§‹æ•°æ®
    if stock_df in stock_data:
        see_data = st.expander('æŸ¥çœ‹åŸå§‹æ•°æ® \ View the raw data ğŸ‘‰')
        with see_data:
            st.dataframe(data=selected_stock_df.reset_index(drop=True))
    else:
        st.sidebar.write('æœªçŸ¥æ•°æ®é›†:', stock_df)

 
    stock_name = selected_stock_df['è‚¡ç¥¨åç§°'].iloc[0]
    # åˆ›å»ºStreamlitåº”ç”¨ç¨‹åº
    st.title('{}è‚¡ç¥¨æ•°æ®å…³è”å›¾'.format(stock_name))
    plt.figure(figsize=(8, 6))


    jointplot = sns.jointplot(x='å¼€ç›˜', y='æ”¶ç›˜', data=selected_stock_df, kind='scatter') 
    jointplot.set_axis_labels(f'{stock_name}', f'{stock_name}', fontsize=12)

    st.pyplot()
    st.markdown("")
    st_profile_report(selected_stock_df)


if __name__ == '__main__':

    logging.basicConfig(level=logging.CRITICAL)

    df = load_data()

    main(df)